{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 环境准备\n",
        "**说明**: 安装所需的Python包,包括Google Cloud AI Platform SDK、Google Generative AI SDK和数据集处理工具。这些是运行整个项目的基础依赖。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform google-genai datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 内核重启\n",
        "**说明**: 重启Jupyter内核以确保新安装的包能够正常加载和使用。这是在安装新包后的标准操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Google认证配置\n",
        "**说明**: 检查是否在Google Colab环境中运行,如果是则进行用户认证。这确保了我们有权限访问Google Cloud服务。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 项目初始化\n",
        "**说明**: 设置Google Cloud项目参数,包括项目ID和地理位置,并初始化Generative AI客户端。这些设置是使用Google Cloud服务的必要配置。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "PROJECT_ID = \"baidao-test-666808\"  # @param {type:\"string\", isTemplate: true}\n",
        "if PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### 5. 依赖库导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Vertex AI SDK\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform.metadata import context\n",
        "from google.cloud.aiplatform.metadata import utils as metadata_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import vertexai\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. 存储配置\n",
        "**说明**: 配置Google Cloud Storage存储桶,用于存储训练数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lit30Cktbfvo"
      },
      "outputs": [],
      "source": [
        "# Provide a bucket name\n",
        "BUCKET_NAME = \"sql-create-context\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "print(BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. 数据集处理\n",
        "**说明**: 加载SQL生成数据集并进行训练集、验证集、测试集的划分。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-vS-5Xpn0C_"
      },
      "outputs": [],
      "source": [
        "# 加载数据集\n",
        "ds = load_dataset(\"b-mc2/sql-create-context\")\n",
        "\n",
        "# 获取训练集数据\n",
        "data = ds['train']\n",
        "\n",
        "# 首先将数据分成 train 和 remaining (9:1)\n",
        "train_test = data.train_test_split(test_size=0.2, seed=42)\n",
        "train_data = train_test['train']\n",
        "\n",
        "# 将 remaining 数据再次分成 valid 和 test (1:1)\n",
        "remaining = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
        "valid_data = remaining['train']\n",
        "test_data = remaining['test']\n",
        "\n",
        "# 将数据集转换为DataFrame\n",
        "train_df = pd.DataFrame(train_data)\n",
        "valid_df = pd.DataFrame(valid_data)\n",
        "test_df = pd.DataFrame(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. 数据采样\n",
        "**说明**: 从各个数据集中随机采样10%的数据,以减少计算资源需求并加快训练过程。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xINJpSEqqhSR"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.sample(frac=0.1, random_state=42)\n",
        "valid_df = valid_df.sample(frac=0.1, random_state=42)\n",
        "test_df = test_df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# 打印抽样后的DataFrame大小\n",
        "print(f\"Sampled train DataFrame size: {len(train_df)}\")\n",
        "print(f\"Sampled validation DataFrame size: {len(valid_df)}\")\n",
        "print(f\"Sampled test DataFrame size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkOmXpegA8CW"
      },
      "outputs": [],
      "source": [
        "test_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. 系统提示设置\n",
        "**说明**: 创建系统提示文本，并结合few shot examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HXBgGFZB3hx"
      },
      "outputs": [],
      "source": [
        "row_dataset = random.randint(0, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jdKUBaA4ZOFg"
      },
      "outputs": [],
      "source": [
        "few_shot_examples = test_df.sample(3)\n",
        "dropped_indices = few_shot_examples.index\n",
        "test_df = test_df.drop(dropped_indices)\n",
        "\n",
        "few_shot_prompt = \"\"\n",
        "for _, row in few_shot_examples.iterrows():\n",
        "    few_shot_prompt += (\n",
        "        f\"Context: {row.context}\\nQuestion: {row.question}\\nAnswer: {row.answer}\\n\\n\"\n",
        "    )\n",
        "\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uJSrdU3aDi9"
      },
      "outputs": [],
      "source": [
        "systemInstruct = f\"\"\"You are a Database Query Assistant. Your role is to generate SQL queries based on user questions and the provided context. The context will be provided in the form of CREATE TABLE statements that define the database schema.\\n\\n\n",
        "Here are some examples: \\n\\n\n",
        "{few_shot_prompt}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N_u3VzUMsyqj"
      },
      "outputs": [],
      "source": [
        "test_df[\"systemInstruct\"] = systemInstruct\n",
        "\n",
        "test_df[\"input_question\"] = (\n",
        "    \"\\n\\n**Below the question with context that you need to answer**\"\n",
        "    + \"\\nContext: \" + test_df[\"context\"]\n",
        "    + \"\\nQuestion: \" + test_df[\"question\"]\n",
        ")\n",
        "\n",
        "test_systemInstruct = test_df[\"systemInstruct\"].iloc[row_dataset]\n",
        "print(test_systemInstruct)\n",
        "test_question = test_df[\"input_question\"].iloc[row_dataset]\n",
        "print(test_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-5X4goiqqBQ"
      },
      "outputs": [],
      "source": [
        "base_model = \"gemini-2.0-flash-lite-001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udTxzY8mpGYf"
      },
      "outputs": [],
      "source": [
        "def get_predictions(question: str, model_version: str) -> str:\n",
        "\n",
        "    prompt = question\n",
        "    base_model = model_version\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=base_model,\n",
        "        contents=prompt,\n",
        "        config={\n",
        "            \"system_instruction\": systemInstruct,\n",
        "            \"temperature\": 0.3,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PFvwmGll3MIv"
      },
      "outputs": [],
      "source": [
        "test_answer = test_df[\"answer\"].iloc[row_dataset]\n",
        "response = get_predictions(test_question, base_model)\n",
        "\n",
        "print(f\"Gemini response: {response}\")\n",
        "print(f\"Actual answer: {test_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.测试原版模型结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQEEaRZZFgCD"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "test_df[\"predicted_answer\"] = test_df[\"input_question\"].progress_apply(lambda x: get_predictions(x, base_model))\n",
        "test_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11. 评估指标函数\n",
        "**说明**: 实现评估模型性能的指标计算函数,包括F1分数和精确匹配分数。这些指标用于衡量模型生成的SQL查询的质量。\n",
        "\n",
        "精确匹配分数（EM Score）：\n",
        "完全相同，则得分为1，否则为0。最终的EM分数是所有测试样本的平均值，直接反映了模型生成完全正确查询的比例。\n",
        "\n",
        "F1分数（F1 Score）：\n",
        "将结果拆分成tokens，然后通过计算预测查询和真实查询中共同出现的词元数量，分别计算精确率（预测中正确词元的占比）和召回率（真实查询中被正确预测的词元占比），最后取这两个指标的调和平均数。即使查询不完全匹配，也能得到一个介于0到1之间的分数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AI57wAgIB6A"
      },
      "outputs": [],
      "source": [
        "def f1_score_squad(prediction, ground_truth):\n",
        "    prediction_tokens = prediction.split()\n",
        "    ground_truth_tokens = ground_truth.split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return prediction == ground_truth\n",
        "\n",
        "\n",
        "def calculate_em_and_f1(y_true, y_pred):\n",
        "    \"\"\"Calculates EM and F1 scores for DataFrame columns.\"\"\"\n",
        "\n",
        "    # Ensure inputs are Series\n",
        "    if not isinstance(y_true, pd.Series):\n",
        "        y_true = pd.Series(y_true)\n",
        "    if not isinstance(y_pred, pd.Series):\n",
        "        y_pred = pd.Series(y_pred)\n",
        "\n",
        "    em = np.mean(y_true.combine(y_pred, exact_match_score))\n",
        "    f1 = np.mean(y_true.combine(y_pred, f1_score_squad))\n",
        "\n",
        "    return em, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12.原版模型结果分数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuHUevIX_fMn"
      },
      "outputs": [],
      "source": [
        "em, f1 = calculate_em_and_f1(test_df[\"answer\"], test_df[\"predicted_answer\"])\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13.处理训练数据\n",
        "**说明**: 训练数据转化成jsonl文件并上传到GCS桶中，以方便后续微调任务，jsonl的格式是适合微调的数据格式"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DqrQp4cLqRy"
      },
      "outputs": [],
      "source": [
        "train_df[\"input_question\"] = (\n",
        "    \"\\n\\n**Below the question with context that you need to answer**\"\n",
        "    + \"\\nContext: \"\n",
        "    + train_df[\"context\"]\n",
        "    + \"\\nQuestion: \"\n",
        "    + train_df[\"question\"]\n",
        ")\n",
        "valid_df[\"input_question\"] = (\n",
        "    \"\\n\\n**Below the question with context that you need to answer**\"\n",
        "    + \"\\nContext: \"\n",
        "    + valid_df[\"context\"]\n",
        "    + \"\\nQuestion: \"\n",
        "    + valid_df[\"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcnD1OiezZiT"
      },
      "outputs": [],
      "source": [
        "train_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmzyz1migvHN"
      },
      "outputs": [],
      "source": [
        "def df_to_jsonl(df, output_file):\n",
        "    \"\"\"Converts a Pandas DataFrame to JSONL format and saves it to a file.\n",
        "\n",
        "    Args:\n",
        "      df: The DataFrame to convert.\n",
        "      output_file: The name of the output file.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for row in df.itertuples(index=False):\n",
        "            jsonl_obj = {\n",
        "                \"systemInstruction\": {\"parts\": [{\"text\": f\"{systemInstruct}\"}]},\n",
        "                \"contents\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"parts\": [{\"text\": f\"{row.input_question}\"}],\n",
        "                    },\n",
        "                    {\"role\": \"model\", \"parts\": [{\"text\": row.answer}]},\n",
        "                ],\n",
        "            }\n",
        "            f.write(json.dumps(jsonl_obj) + \"\\n\")\n",
        "\n",
        "\n",
        "# Process the DataFrames\n",
        "df_to_jsonl(train_df, \"train.jsonl\")\n",
        "df_to_jsonl(valid_df, \"valid.jsonl\")\n",
        "\n",
        "print(f\"JSONL data written to train.jsonl\")\n",
        "print(f\"JSONL data written to valid.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQv-ZMpJDhi"
      },
      "source": [
        "复制数据到桶中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5k1jYJ10IeW"
      },
      "outputs": [],
      "source": [
        "!gsutil cp ./train.jsonl {BUCKET_URI}\n",
        "!gsutil cp ./valid.jsonl {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 14.启动微调工作\n",
        "\n",
        "- `base_model`: 想要微调的模型\n",
        " - `train_dataset`: 训练数据集路径\n",
        "\n",
        "  *Optional parameters*\n",
        " - `epochs`: 训练轮数.\n",
        " - `learning_rate_multiplier`: 学习率倍率.\n",
        " - `adapter_size` : 秩."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdcy4umfpGZE"
      },
      "outputs": [],
      "source": [
        "train_dataset = f\"\"\"{BUCKET_URI}/train.jsonl\"\"\"\n",
        "validation_dataset = f\"\"\"{BUCKET_URI}/valid.jsonl\"\"\"\n",
        "\n",
        "training_dataset = {\n",
        "    \"gcs_uri\": train_dataset,\n",
        "}\n",
        "\n",
        "validation_dataset = types.TuningValidationDataset(gcs_uri=validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkboVUkoqWSp"
      },
      "outputs": [],
      "source": [
        "sft_tuning_job = client.tunings.tune(\n",
        "    base_model=base_model,\n",
        "    training_dataset=training_dataset,\n",
        "    config=types.CreateTuningJobConfig(\n",
        "        adapter_size=\"ADAPTER_SIZE_EIGHT\",\n",
        "        epoch_count=1,  # set to one to keep time and cost low\n",
        "        tuned_model_display_name=\"sql-Tuned\",\n",
        "        validation_dataset=validation_dataset,\n",
        "    ),\n",
        ")\n",
        "sft_tuning_job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WECSLyPRth6M"
      },
      "outputs": [],
      "source": [
        "sft_tuning_job.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iwz4lhUDC_f"
      },
      "outputs": [],
      "source": [
        "tuning_job = client.tunings.get(name=sft_tuning_job.name)\n",
        "tuning_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 15.模型调优指标\n",
        "\n",
        "- `/train_total_loss`: 训练步骤中调优数据集的损失值。\n",
        "- `/train_fraction_of_correct_next_step_preds`: 训练步骤中的标记准确率。一个预测由一系列标记组成。此指标衡量预测标记与调优数据集中的真实值相比的准确度。\n",
        "- `/train_num_predictions`:训练步骤中预测的标记数量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IoiiRH5Lhpf"
      },
      "outputs": [],
      "source": [
        "experiment_name = tuning_job.experiment\n",
        "experiment_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH0guHM---Jo"
      },
      "outputs": [],
      "source": [
        "# Locate Vertex AI Experiment and Vertex AI Experiment Run\n",
        "experiment = aiplatform.Experiment(experiment_name=experiment_name)\n",
        "filter_str = metadata_utils._make_filter_string(\n",
        "    schema_title=\"system.ExperimentRun\",\n",
        "    parent_contexts=[experiment.resource_name],\n",
        ")\n",
        "experiment_run = context.Context.list(filter_str)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hggHQFIl_FXC"
      },
      "outputs": [],
      "source": [
        "# Read data from Tensorboard\n",
        "tensorboard_run_name = f\"{experiment.get_backing_tensorboard_resource().resource_name}/experiments/{experiment.name}/runs/{experiment_run.name.replace(experiment.name, '')[1:]}\"\n",
        "tensorboard_run = aiplatform.TensorboardRun(tensorboard_run_name)\n",
        "metrics = tensorboard_run.read_time_series_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdHKZdqG_bHf"
      },
      "outputs": [],
      "source": [
        "def get_metrics(metric: str = \"/train_total_loss\"):\n",
        "    \"\"\"\n",
        "    Get metrics from Tensorboard.\n",
        "\n",
        "    Args:\n",
        "      metric: metric name, eg. /train_total_loss or /eval_total_loss.\n",
        "    Returns:\n",
        "      steps: list of steps.\n",
        "      steps_loss: list of loss values.\n",
        "    \"\"\"\n",
        "    loss_values = metrics[metric].values\n",
        "    steps_loss = []\n",
        "    steps = []\n",
        "    for loss in loss_values:\n",
        "        steps_loss.append(loss.scalar.value)\n",
        "        steps.append(loss.step)\n",
        "    return steps, steps_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pDrlpA7_e9o"
      },
      "outputs": [],
      "source": [
        "# Get Train and Eval Loss\n",
        "train_loss = get_metrics(metric=\"/train_total_loss\")\n",
        "eval_loss = get_metrics(metric=\"/eval_total_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL07j7u__iZx"
      },
      "outputs": [],
      "source": [
        "# Plot the train and eval loss metrics using Plotly python library\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2, shared_xaxes=True, subplot_titles=(\"Train Loss\", \"Eval Loss\")\n",
        ")\n",
        "\n",
        "# Add traces\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=train_loss[0], y=train_loss[1], name=\"Train Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=1,\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=eval_loss[0], y=eval_loss[1], name=\"Eval Loss\", mode=\"lines\"),\n",
        "    row=1,\n",
        "    col=2,\n",
        ")\n",
        "\n",
        "# Add figure title\n",
        "fig.update_layout(title=\"Train and Eval Loss\", xaxis_title=\"Steps\", yaxis_title=\"Loss\")\n",
        "\n",
        "# Set x-axis title\n",
        "fig.update_xaxes(title_text=\"Steps\")\n",
        "\n",
        "# Set y-axes titles\n",
        "fig.update_yaxes(title_text=\"Loss\")\n",
        "\n",
        "# Show plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pivmh4Lwbgy1"
      },
      "source": [
        "### 16.使用微调后的模型评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO6ln4teagw1"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Answer the question based on the context\n",
        "\n",
        "question: What is the branding for callsign dypv?,\n",
        "context: CREATE TABLE table_27588823_2 (branding VARCHAR, callsign VARCHAR)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJgK_ZfL5dsj"
      },
      "outputs": [],
      "source": [
        "tuned_model = tuning_job.tuned_model.endpoint\n",
        "tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifhRboiCOBje"
      },
      "outputs": [],
      "source": [
        "get_predictions(prompt, tuned_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyrEDGAoA4f5"
      },
      "outputs": [],
      "source": [
        "# Apply the get_prediction() function to the 'question_column'\n",
        "test_df[\"predicted_answer\"] = test_df[\"input_question\"].progress_apply(lambda x: get_predictions(x, tuned_model))\n",
        "test_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "运行评估后，您可以看到经过微调的模型在我们的使用场景中整体表现更好。当然，具体性能会因使用场景或数据质量等因素而有所不同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjoRgjg6fr0P"
      },
      "outputs": [],
      "source": [
        "em, f1 = calculate_em_and_f1(test_df[\"answer\"], test_df[\"predicted_answer\"])\n",
        "print(f\"EM score: {em}\")\n",
        "print(f\"F1 score: {f1}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "sft_gemini_qa2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
